<!DOCTYPE html><!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]--><!--[if gt IE 8]><!--><html xmlns="http://www.w3.org/1999/xhtml" class=" js flexbox canvas canvastext webgl no-touch geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths" lang="en" style=""><!--<![endif]--><head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Downloader Middleware — Scrapy 0.18.4 documentation</title>
  

  
  

  

  
  
    

  

  
  

  
    <link rel="stylesheet" href="https://media.readthedocs.org/css/sphinx_rtd_theme.css" type="text/css" />
  

  
    <link rel="top" title="Scrapy 0.18.4 documentation" href="../index.html" />
        <link rel="next" title="Spider Middleware" href="spider-middleware.html" />
        <link rel="prev" title="Architecture overview" href="architecture.html" /> 

  
  <script type="text/javascript" async="" src="https://ssl.google-analytics.com/ga.js"></script><script src="../_static/js/modernizr.min.js"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="../_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'topics/downloader-middleware'
</script>

<script type="text/javascript" src="../_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>

<body class="wy-body-for-nav" role="document" style="">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Scrapy
          

          
          </a>

          
            
            
            
              <div class="version">
                0.18
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Scrapy at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/install.html">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/tutorial.html">Scrapy Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="commands.html">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="items.html">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="spiders.html">Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="selectors.html">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="loaders.html">Item Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="shell.html">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="item-pipeline.html">Item Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="feed-exports.html">Feed exports</a></li>
<li class="toctree-l1"><a class="reference internal" href="link-extractors.html">Link Extractors</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Stats Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="email.html">Sending e-mail</a></li>
<li class="toctree-l1"><a class="reference internal" href="telnetconsole.html">Telnet Console</a></li>
<li class="toctree-l1"><a class="reference internal" href="webservice.html">Web Service</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="contracts.html">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="practices.html">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="broad-crawls.html">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="firefox.html">Using Firefox for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="firebug.html">Using Firebug for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="leaks.html">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="images.html">Downloading Item Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="ubuntu.html">Ubuntu packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="scrapyd.html">Scrapyd</a></li>
<li class="toctree-l1"><a class="reference internal" href="autothrottle.html">AutoThrottle extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">Jobs: pausing and resuming crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="djangoitem.html">DjangoItem</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href=""><span class="toctree-expand"></span>Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#writing-your-own-downloader-middleware">Writing your own downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#built-in-downloader-middleware-reference"><span class="toctree-expand"></span>Built-in downloader middleware reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.cookies"><span class="toctree-expand"></span>CookiesMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multiple-cookie-sessions-per-spider">Multiple cookie sessions per spider</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cookies-enabled">COOKIES_ENABLED</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cookies-debug">COOKIES_DEBUG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.defaultheaders">DefaultHeadersMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.downloadtimeout">DownloadTimeoutMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.httpauth">HttpAuthMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.httpcache"><span class="toctree-expand"></span>HttpCacheMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dummy-policy-default">Dummy policy (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rfc2616-policy">RFC2616 policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dbm-storage-backend-default">DBM storage backend (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#filesystem-storage-backend">Filesystem storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#httpcache-middleware-settings">HTTPCache middleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.httpcompression"><span class="toctree-expand"></span>HttpCompressionMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#httpcompressionmiddleware-settings">HttpCompressionMiddleware Settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.chunked">ChunkedTransferMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.httpproxy">HttpProxyMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.redirect"><span class="toctree-expand"></span>RedirectMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#redirectmiddleware-settings">RedirectMiddleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#metarefreshmiddleware"><span class="toctree-expand"></span>MetaRefreshMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#metarefreshmiddleware-settings">MetaRefreshMiddleware settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.retry"><span class="toctree-expand"></span>RetryMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#retrymiddleware-settings">RetryMiddleware Settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.robotstxt">RobotsTxtMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.stats">DownloaderStats</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-scrapy.contrib.downloadermiddleware.useragent">UserAgentMiddleware</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="spider-middleware.html">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="extensions.html">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Core API</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="request-response.html">Requests and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="settings.html">Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="signals.html">Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">Exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="exporters.html">Item Exporters</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versioning.html">Versioning and API Stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experimental/index.html">Experimental features</a></li>
</ul>

            
          
        </div>
      <div id="rtd-r0sv1lbq" class="ethical-rtd ethical-dark-theme"><div class="ethical-sidebar"><div class="ethical-content"><a href="https://readthedocs.org/sustainability/click/305/13BxTiqHbGJI/" rel="nofollow" target="_blank" class="ethical-image-link"><img src="https://assets.readthedocs.org/sustainability/readthedocs-logo-fs8.png" /></a><div class="ethical-text"><a href="https://readthedocs.org/sustainability/click/305/13BxTiqHbGJI/" rel="nofollow" target="_blank">Private repos and priority support<br />Try Read the Docs for Business Today!</a></div></div><div class="ethical-callout"><small><em><a href="https://readthedocs.org/sustainability/advertising/">Sponsored</a><span> · </span><a href="https://docs.readthedocs.io/en/latest/ethical-advertising.html">Ads served ethically</a></em></small></div></div></div></div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> »</li>
      
    <li>Downloader Middleware</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="https://github.com/scrapy/scrapy/blob/0.18/docs/topics/downloader-middleware.rst" class="fa fa-github"> Edit on GitHub</a>
          
        
      </li>
  </ul>
  <hr />
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h1>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this headline">¶</a></h1>
<p>The downloader middleware is a framework of hooks into Scrapy’s
request/response processing.  It’s a light, low-level system for globally
altering Scrapy’s requests and responses.</p>
<div class="section" id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h2>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this headline">¶</a></h2>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'myproject.middlewares.CustomDownloaderMiddleware'</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the downloader.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project’s <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <cite>None</cite>
as its value.  For example, if you want to disable the off-site middleware:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'myproject.middlewares.CustomDownloaderMiddleware'</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s1">'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-downloader-middleware">
<h2>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this headline">¶</a></h2>
<p>Writing your own downloader middleware is easy. Each middleware component is a
single Python class that defines one or more of the following methods:</p>
<span class="target" id="module-scrapy.contrib.downloadermiddleware"></span><dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.</code><code class="descname">DownloaderMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request">
<code class="descname">process_request</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> should return either <code class="docutils literal"><span class="pre">None</span></code>, a
<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, or a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, Scrapy won’t bother
calling ANY other request or exception middleware, or the appropriate
download function; it’ll return that Response. Response middleware is
always called on every Response.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the
entire request will be dropped completely and its callback never called.</p>
<table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name" />
<col class="field-body" />
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) – the request being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.BaseSpider" title="scrapy.spider.BaseSpider"><code class="xref py py-class docutils literal"><span class="pre">BaseSpider</span></code></a> object) – the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response">
<code class="descname">process_response</code><span class="sig-paren">(</span><em>request</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> should return either a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>
object, a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object or
raise a <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> of the next middleware in the pipeline.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the returned request will be
rescheduled to be downloaded in the future.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the
response will be dropped completely and its callback never called.</p>
<table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name" />
<col class="field-body" />
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) – the request that originated the response</li>
<li><strong>response</strong> (<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) – the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.BaseSpider" title="scrapy.spider.BaseSpider"><code class="xref py py-class docutils literal"><span class="pre">BaseSpider</span></code></a> object) – the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception">
<code class="descname">process_exception</code><span class="sig-paren">(</span><em>request</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception.</p>
<p><a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> should return either <code class="docutils literal"><span class="pre">None</span></code>,
<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> or <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other exception middleware, until no middleware is left and
the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, the response middleware
kicks in, and won’t bother calling any other exception middleware.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the returned request is
used to instruct an immediate redirection.
The original request won’t finish until the redirected
request is completed. This stops the <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception" title="scrapy.contrib.downloadermiddleware.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a>
middleware the same as returning Response would do.</p>
<table class="docutils field-list" frame="void" rules="none">
<colgroup><col class="field-name" />
<col class="field-body" />
</colgroup><tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) – the request that generated the exception</li>
<li><strong>exception</strong> (an <code class="docutils literal"><span class="pre">Exception</span></code> object) – the raised exception</li>
<li><strong>spider</strong> (<a class="reference internal" href="spiders.html#scrapy.spider.BaseSpider" title="scrapy.spider.BaseSpider"><code class="xref py py-class docutils literal"><span class="pre">BaseSpider</span></code></a> object) – the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h2>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this headline">¶</a></h2>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="#topics-downloader-middleware"><span>downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h3>CookiesMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.cookies" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.cookies.</code><code class="descname">CookiesMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a></li>
</ul>
<div class="section" id="multiple-cookie-sessions-per-spider">
<span id="std:reqmeta-cookiejar"></span><h4>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this headline">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="s2">"http://www.example.com"</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">'cookiejar'</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that the <a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># do some processing</span>
    <span class="k">return</span> <span class="n">Request</span><span class="p">(</span><span class="s2">"http://www.example.com/otherpage"</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">'cookiejar'</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">'cookiejar'</span><span class="p">]},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_other_page</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h4>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this headline">¶</a></h4>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h4>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this headline">¶</a></h4>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <code class="docutils literal"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (ie. <code class="docutils literal"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here’s an example of a log with <a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>2011-04-06 14:35:10-0300 [diningcity] INFO: Spider opened
2011-04-06 14:35:10-0300 [diningcity] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [diningcity] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [diningcity] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.defaultheaders">
<span id="defaultheadersmiddleware"></span><h3>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.defaultheaders" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.defaultheaders.</code><code class="descname">DefaultHeadersMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="settings.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h3>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.downloadtimeout" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.downloadtimeout.</code><code class="descname">DownloadTimeoutMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="settings.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpauth">
<span id="httpauthmiddleware"></span><h3>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpauth" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.httpauth.</code><code class="descname">HttpAuthMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="http://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <code class="docutils literal"><span class="pre">http_user</span></code>
and <code class="docutils literal"><span class="pre">http_pass</span></code> attributes of those spiders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SomeIntranetSiteSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">http_user</span> <span class="o">=</span> <span class="s1">'someuser'</span>
    <span class="n">http_pass</span> <span class="o">=</span> <span class="s1">'somepass'</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">'intranet.example.com'</span>

    <span class="c1"># .. rest of the spider code omitted ...</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcache">
<span id="httpcachemiddleware"></span><h3>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcache" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.httpcache.</code><code class="descname">HttpCacheMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with two HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-storage-dbm"><span>DBM storage backend (default)</span></a></li>
<li><a class="reference internal" href="#httpcache-storage-fs"><span>Filesystem storage backend</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also implement your own storage backend.</p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-policy-rfc2616"><span>RFC2616 policy</span></a></li>
<li><a class="reference internal" href="#httpcache-policy-dummy"><span>Dummy policy (default)</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
</dd></dl>

<div class="section" id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h4>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this headline">¶</a></h4>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DummyPolicy</span></code></li>
</ul>
</div>
<div class="section" id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h4>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this headline">¶</a></h4>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<ul class="simple">
<li>Do not attempt to store responses/requests with <cite>no-store</cite> cache-control directive set</li>
<li>Do not serve responses from cache if <cite>no-cache</cite> cache-control directive is set even for fresh responses</li>
<li>Compute freshness lifetime from <cite>max-age</cite> cache-control directive</li>
<li>Compute freshness lifetime from <cite>Expires</cite> response header</li>
<li>Compute freshness lifetime from <cite>Last-Modified</cite> response header (heuristic used by Firefox)</li>
<li>Compute current age from <cite>Age</cite> response header</li>
<li>Compute current age from <cite>Date</cite> header</li>
<li>Revalidate stale responses based on <cite>Last-Modified</cite> response header</li>
<li>Revalidate stale responses based on <cite>ETag</cite> response header</li>
<li>Set <cite>Date</cite> header for any received response missing it</li>
</ul>
<p>what is missing:</p>
<ul class="simple">
<li><cite>Pragma: no-cache</cite> support <a class="reference external" href="http://www.mnot.net/cache_docs/#PRAGMA">http://www.mnot.net/cache_docs/#PRAGMA</a></li>
<li><cite>Vary</cite> header support <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>Invalidation after updates or deletes <a class="reference external" href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>... probably others ..</li>
</ul>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.contrib.httpcache.RFC2616Policy</span></code></li>
</ul>
</div>
<div class="section" id="dbm-storage-backend-default">
<span id="httpcache-storage-dbm"></span><h4>DBM storage backend (default)<a class="headerlink" href="#dbm-storage-backend-default" title="Permalink to this headline">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>A <a class="reference external" href="http://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="http://docs.python.org/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="#std:setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></code></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend">
<span id="httpcache-storage-fs"></span><h4>Filesystem storage backend<a class="headerlink" href="#filesystem-storage-backend" title="Permalink to this headline">¶</a></h4>
<p>A file system storage backend is also available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.contrib.httpcache.FilesystemCacheStorage</span></code></li>
</ul>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">request_body</span></code> - the plain request body</li>
<li><code class="docutils literal"><span class="pre">request_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal"><span class="pre">response_body</span></code> - the plain response body</li>
<li><code class="docutils literal"><span class="pre">response_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal"><span class="pre">meta</span></code> - some metadata of this cache resource in Python <code class="docutils literal"><span class="pre">repr()</span></code> format
(grep-friendly format)</li>
<li><code class="docutils literal"><span class="pre">pickled_meta</span></code> - the same metadata in <code class="docutils literal"><span class="pre">meta</span></code> but pickled for more
efficient deserialization</li>
</ul>
</div></blockquote>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</pre></div>
</div>
</div>
<div class="section" id="httpcache-middleware-settings">
<h4>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware" title="scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></code></a> can be configured through the following
settings:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h5>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="#std:setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></code></a> was used to enable cache.</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h5>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h5>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="commands.html#topics-project-structure"><span>Default structure of Scrapy projects</span></a>.</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h5>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>Don’t cache response with these HTTP codes.</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h5>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h5>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">['file']</span></code></p>
<p>Don’t cache responses with these URI schemes.</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h5>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.DbmCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h5>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">'anydbm'</span></code></p>
<p>The database module to use in the <a class="reference internal" href="#httpcache-storage-dbm"><span>DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h5>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.contrib.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpcompression">
<span id="httpcompressionmiddleware"></span><h3>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpcompression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.httpcompression.</code><code class="descname">HttpCompressionMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h4>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h5>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.chunked">
<span id="chunkedtransfermiddleware"></span><h3>ChunkedTransferMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.chunked" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.chunked.</code><code class="descname">ChunkedTransferMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware adds support for <a class="reference external" href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.httpproxy">
<span id="httpproxymiddleware"></span><h3>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.httpproxy" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.8.</span></p>
</div>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.httpproxy.</code><code class="descname">HttpProxyMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal"><span class="pre">proxy</span></code> meta value to <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="http://docs.python.org/library/urllib.html">urllib</a> and <a class="reference external" href="http://docs.python.org/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">http_proxy</span></code></li>
<li><code class="docutils literal"><span class="pre">https_proxy</span></code></li>
<li><code class="docutils literal"><span class="pre">no_proxy</span></code></li>
</ul>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.redirect">
<span id="redirectmiddleware"></span><h3>RedirectMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.redirect" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</code><code class="descname">RedirectMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal"><span class="pre">redirect_urls</span></code> <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> key.</p>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></code></a></li>
<li><a class="reference internal" href="settings.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></code></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> contains the
<code class="docutils literal"><span class="pre">dont_redirect</span></code> key, the request will be ignored by this middleware.</p>
<div class="section" id="redirectmiddleware-settings">
<h4>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h5>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be follow for a single request.</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h3>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.redirect.</code><code class="descname">MetaRefreshMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-METAREFRESH_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_ENABLED</span></code></a></li>
<li><code class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_MAXDELAY</span></code></li>
</ul>
<p>This middleware obey <a class="reference internal" href="settings.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="reference internal" href="#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></code></a>
and <a class="reference internal" href="#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></code></a> request meta keys as described for <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware" title="scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a></p>
<div class="section" id="metarefreshmiddleware-settings">
<h4>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h5>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-metarefresh-delay">
<span id="std:setting-REDIRECT_MAX_METAREFRESH_DELAY"></span><h5>REDIRECT_MAX_METAREFRESH_DELAY<a class="headerlink" href="#redirect-max-metarefresh-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.retry">
<span id="retrymiddleware"></span><h3>RetryMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.retry" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.retry.</code><code class="descname">RetryMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middlware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.</p>
<p>The <a class="reference internal" href="#scrapy.contrib.downloadermiddleware.retry.RetryMiddleware" title="scrapy.contrib.downloadermiddleware.retry.RetryMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a></li>
</ul>
<p>About HTTP errors to consider:</p>
<p>You may want to remove 400 from <a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a>, if you stick to the
HTTP protocol. It’s included by default because it’s a common code used
to indicate server overload, which would be something we want to retry.</p>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> contains the <code class="docutils literal"><span class="pre">dont_retry</span></code>
key, the request will be ignored by this middleware.</p>
<div class="section" id="retrymiddleware-settings">
<h4>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this headline">¶</a></h4>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h5>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h5>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h5>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">400,</span> <span class="pre">408]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h3>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.robotstxt" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.robotstxt.</code><code class="descname">RobotsTxtMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="settings.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Keep in mind that, if you crawl using multiple concurrent
requests per domain, Scrapy could still  download some forbidden pages
if they were requested before the robots.txt file was downloaded. This
is a known limitation of the current robots.txt middleware and will
be fixed in the future.</p>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.stats">
<span id="downloaderstats"></span><h3>DownloaderStats<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.stats" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.stats.DownloaderStats">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.stats.</code><code class="descname">DownloaderStats</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.contrib.downloadermiddleware.useragent">
<span id="useragentmiddleware"></span><h3>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.contrib.downloadermiddleware.useragent" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.contrib.downloadermiddleware.useragent.</code><code class="descname">UserAgentMiddleware</code><a class="headerlink" href="#scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <cite>user_agent</cite>
attribute must be set.</p>
</dd></dl>

</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="spider-middleware.html" class="btn btn-neutral float-right" title="Spider Middleware" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="architecture.html" class="btn btn-neutral" title="Architecture overview" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr /><div><div id="rtd-r8wiykud" class="ethical-rtd"></div></div>

  <div role="contentinfo">
    <p>
        © Copyright 2008-2013, Scrapy developers.
      
        <span class="commit">
          Revision <code>dc43890b</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: 0.18
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
          <dd><a href="/en/master/">master</a></dd>
        
          <dd><a href="/en/1.1/">1.1</a></dd>
        
          <dd><a href="/en/1.0/">1.0</a></dd>
        
          <dd><a href="/en/0.24/">0.24</a></dd>
        
          <dd><a href="/en/0.22/">0.22</a></dd>
        
          <dd><a href="/en/0.20/">0.20</a></dd>
        
          <dd><a href="/en/0.18/">0.18</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/pdf/0.18/">pdf</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/htmlzip/0.18/">htmlzip</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/epub/0.18/">epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr />
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.18.4',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/jquery/jquery-2.0.3.min.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/jquery/jquery-migrate-1.2.1.min.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/underscore.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/doctools.js"></script>
      <script type="text/javascript" src="https://media.readthedocs.org/javascript/readthedocs-doc-embed.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   


<div id="rtd-819g1tn3"></div></body></html>